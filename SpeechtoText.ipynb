{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "htkcBCneZ11r",
    "outputId": "adbff3c1-230d-44e2-fb03-ed434e19a671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: torchaudio in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: librosa in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: filelock in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from librosa) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from librosa) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: joblib>=0.14 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from librosa) (0.58.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from librosa) (1.8.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from librosa) (1.0.7)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from pooch>=1.0->librosa) (4.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from pooch>=1.0->librosa) (23.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pycparser in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchaudio librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qGHvi2wEh4O8",
    "outputId": "ab54f44f-81d8-48d1-af51-deebfc55f9fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from SpeechRecognition) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2023.11.17)\n",
      "Requirement already satisfied: pydub in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: ffmpeg in d:\\projects\\voice cloning\\voicecloning2\\venv\\lib\\site-packages (1.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install SpeechRecognition\n",
    "! pip install pydub\n",
    "! pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in recordings:\n",
      "Bait.mp3\n",
      "Bait.wav\n",
      "Intro.mp3\n",
      "Video 1 (6).mp3\n",
      "Video 2 (7-22).mp3\n",
      "Video 3.mp3\n",
      "Video 4.mp3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def print_files_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Print the names of all files present in the given folder.\n",
    "\n",
    "    Args:\n",
    "      folder_path: The path to the folder.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return\n",
    "\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"{folder_path} is not a directory.\")\n",
    "        return\n",
    "\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    if not files:\n",
    "        print(f\"No files found in {folder_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Files in {folder_path}:\")\n",
    "    for file_name in files:\n",
    "        print(file_name)\n",
    "\n",
    "# Example usage\n",
    "folder_path = 'recordings'  # Replace with the path to your folder\n",
    "print_files_in_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linux only code\n",
    "# from pydub import AudioSegment\n",
    "# import os\n",
    "\n",
    "# def convert_mp3_to_wav(directory):\n",
    "#     \"\"\"\n",
    "#     Convert all MP3 files in the given directory to WAV format.\n",
    "\n",
    "#     Args:\n",
    "#       directory: The path to the directory containing MP3 files.\n",
    "#     \"\"\"\n",
    "#     for filename in os.listdir(directory):\n",
    "#         if filename.endswith(\".mp3\"):\n",
    "#             mp3_path = os.path.join(directory, filename)\n",
    "#             wav_path = os.path.join(directory, filename.replace(\".mp3\", \".wav\"))\n",
    "\n",
    "#             # Convert MP3 to WAV\n",
    "#             audio = AudioSegment.from_mp3(mp3_path)\n",
    "#             audio.export(wav_path, format=\"wav\")\n",
    "#             print(f\"Converted {filename} to WAV format\")\n",
    "\n",
    "# # Example usage\n",
    "# convert_mp3_to_wav('recordings')  # Replace with your directory path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import speech_recognition as sr\n",
    "import json\n",
    "\n",
    "def transcribe_audio(wav_path):\n",
    "    \"\"\"\n",
    "    Transcribe the given WAV audio file to text.\n",
    "\n",
    "    Args:\n",
    "      wav_path: The path to the WAV audio file.\n",
    "\n",
    "    Returns:\n",
    "      A string containing the transcribed text.\n",
    "    \"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(wav_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            return \"Audio Unintelligible\"\n",
    "        except sr.RequestError:\n",
    "            return \"Request Failed\"\n",
    "\n",
    "def create_transcripts_file(audio_folder_path):\n",
    "    \"\"\"\n",
    "    Given an audio folder path, create the 'transcripts.json' file in the current working directory.\n",
    "\n",
    "    Args:\n",
    "      audio_folder_path: The path to the audio folder.\n",
    "    \"\"\"\n",
    "\n",
    "    transcripts_file_path = 'transcripts.json'\n",
    "    transcripts = {}\n",
    "    for audio_file in os.listdir(audio_folder_path):\n",
    "        if audio_file.endswith('.wav'):\n",
    "            wav_path = os.path.join(audio_folder_path, audio_file)\n",
    "            transcripts[audio_file] = transcribe_audio(wav_path)\n",
    "\n",
    "    with open(transcripts_file_path, 'w') as f:\n",
    "        json.dump(transcripts, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "create_transcripts_file('recordings')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XIN8qC8xfts8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import json\n",
    "\n",
    "def preprocess_data(audio_folder, transcript_file):\n",
    "    data = []\n",
    "    with open(transcript_file, 'r') as f:\n",
    "        transcripts = json.load(f)\n",
    "\n",
    "    for filename in os.listdir(audio_folder):\n",
    "        if filename.endswith('.wav'):\n",
    "            transcript = transcripts.get(filename)\n",
    "            if transcript:\n",
    "                path = os.path.join(audio_folder, filename)\n",
    "                audio, sr = librosa.load(path, sr=None)\n",
    "                data.append({'audio': audio, 'transcript': transcript, 'sampling_rate': sr})\n",
    "\n",
    "    return data\n",
    "\n",
    "dataset = preprocess_data('recordings', 'transcripts.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'audio': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), 'transcript': 'get your free LinkedIn no good AI generated headshot snap no need to spend hours to get ready to pay a photographer to get the headshot script Di is here to save you with the bonus free prompt at the end head to update gaana.com now', 'sampling_rate': 48000}]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[:5])  # Print the first 5 elements of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jGhEc8yRi2tJ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleTTSModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleTTSModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=100, hidden_size=128, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)  # Adjust output size according to your data\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GBF_FzTdi6T5"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming you have a model defined as `SimpleTTSModel`\n",
    "# # and your dataset prepared as `dataset`\n",
    "\n",
    "# # A function to convert text to a tensor (this is a placeholder, you'll need to define this based on your model's needs)\n",
    "# def text_to_tensor(text):\n",
    "#     # This function should convert text to a tensor\n",
    "#     # For simplicity, let's assume each character is converted to an ASCII value\n",
    "#     return torch.tensor([ord(c) for c in text], dtype=torch.float32)\n",
    "\n",
    "# # A function to prepare your batch\n",
    "# def prepare_batch(batch):\n",
    "#     audio_tensors = [torch.tensor(item['audio'], dtype=torch.float32) for item in batch]\n",
    "#     text_tensors = [text_to_tensor(item['transcript']) for item in batch]\n",
    "\n",
    "#     # Padding sequences to have same length\n",
    "#     audio_tensors = nn.utils.rnn.pad_sequence(audio_tensors, batch_first=True)\n",
    "#     text_tensors = nn.utils.rnn.pad_sequence(text_tensors, batch_first=True)\n",
    "\n",
    "#     return audio_tensors, text_tensors\n",
    "\n",
    "# # Custom Dataset class\n",
    "# class CustomDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         self.data = data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.data[idx]\n",
    "\n",
    "# # Creating a DataLoader\n",
    "# batch_size = 4  # You can modify this based on your dataset and GPU capabilities\n",
    "# train_loader = DataLoader(CustomDataset(dataset), batch_size=batch_size, shuffle=True, collate_fn=prepare_batch)\n",
    "\n",
    "# # Training function\n",
    "# def train(model, train_loader, epochs):\n",
    "#     model.train()\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         for batch_idx, (audio, transcripts) in enumerate(train_loader):\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # Assuming your model's forward method takes audio and returns predicted transcripts\n",
    "#             predictions = model(audio)\n",
    "\n",
    "#             # Ensure predictions and targets are the same shape\n",
    "#             transcripts = transcripts.to(predictions.device)\n",
    "#             transcripts = nn.utils.rnn.pad_sequence([transcripts[i][:len(predictions[i])] for i in range(len(transcripts))], batch_first=True)\n",
    "\n",
    "#             loss = criterion(predictions, transcripts)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             if batch_idx % 10 == 0:\n",
    "#                 print(f'Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss.item()}')\n",
    "\n",
    "# # Initialize your model\n",
    "# model = SimpleTTSModel()\n",
    "\n",
    "# # Train the model\n",
    "# train(model, train_loader, epochs=10)\n",
    "# torch.save(model.state_dict(), 'tts_model.pth')\n",
    "\n",
    "\n",
    "# model = SimpleTTSModel()\n",
    "# model.load_state_dict(torch.load('tts_model.pth'))\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define your SimpleTTSModel here\n",
    "\n",
    "def text_to_tensor(text):\n",
    "    # Converts text to a tensor\n",
    "    return torch.tensor([ord(c) for c in text], dtype=torch.float32)\n",
    "\n",
    "def prepare_batch(batch):\n",
    "    audio_tensors = [item[0] for item in batch]\n",
    "    text_tensors = [item[1] for item in batch]\n",
    "\n",
    "    audio_tensors = nn.utils.rnn.pad_sequence(audio_tensors, batch_first=True)\n",
    "    text_tensors = nn.utils.rnn.pad_sequence(text_tensors, batch_first=True)\n",
    "\n",
    "    return audio_tensors, text_tensors\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio = torch.tensor(self.dataset[idx]['audio'], dtype=torch.float32)\n",
    "        transcript = text_to_tensor(self.dataset[idx]['transcript'])\n",
    "        return audio, transcript\n",
    "\n",
    "\n",
    "def train(model, train_loader, epochs):\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (audio, transcript) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            audio, transcript = audio.to(device), transcript.to(device)\n",
    "            prediction = model(audio)\n",
    "            prediction = prediction.squeeze(0)\n",
    "            loss = criterion(prediction, transcript)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss.item()}')\n",
    "            del audio, transcript, prediction\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(CustomDataset(dataset), batch_size=batch_size, shuffle=True, collate_fn=prepare_batch)\n",
    "\n",
    "# Define your SimpleTTSModel and train function here\n",
    "model = SimpleTTSModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "train(model, train_loader, epochs=10)\n",
    "\n",
    "torch.save(model.state_dict(), 'tts_model.pth')\n",
    "model.load_state_dict(torch.load('tts_model.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
